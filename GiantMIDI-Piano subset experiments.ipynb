{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:31:58.066866Z",
     "start_time": "2023-06-18T16:31:47.858198200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import winsound\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, precision_score, recall_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HGTConv, SAGEConv, GATConv, Linear, to_hetero\n",
    "\n",
    "from HeteroDataFunctions import Encoder, complete_graph, flatten_lol, node_cat_dict, midi_type, plot_graph, plot_4graphs\n",
    "\n",
    "# print(scipy.__version__)\n",
    "# print(matplotlib.__version__)\n",
    "# print(nx.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 960\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:31:58.080878100Z",
     "start_time": "2023-06-18T16:31:58.067867Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading edgelists...\n",
      "- notes.edgelist\n",
      "- program.edgelist\n",
      "- tempo.edgelist\n",
      "- time.signature.edgelist\n",
      "Nodes: 286550\n",
      "Edges: 2756865\n"
     ]
    }
   ],
   "source": [
    "# Load the complete graph\n",
    "G = complete_graph(\".\\giantmidi-piano\\edgelist\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:32:59.309424900Z",
     "start_time": "2023-06-18T16:31:58.081878100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "nodes = pd.DataFrame((list(G.nodes)), columns=['name'])\n",
    "edges = pd.DataFrame(np.array(list(G.edges)), columns=['source', 'target'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:04.184563600Z",
     "start_time": "2023-06-18T16:32:59.310425800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "       name\n286548   12",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>286548</th>\n      <td>12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = nodes['name'].str.match(r'^-?\\d+(\\.\\d+)?$')\n",
    "\n",
    "nodes[matches]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:04.401747400Z",
     "start_time": "2023-06-18T16:33:04.186565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def node_cat_dict_giant(nodes: pd.DataFrame) -> dict:\n",
    "    \"\"\"Compile all nodes in the nodes Dataframe in a dictionary.\"\"\"\n",
    "    note_groups = [n for n in nodes['name'] if n[0] == 'g' and n[1] in [str(i) for i in range(10)] + ['-']]\n",
    "\n",
    "    # not_group_nodes = [n for n in nodes['name'] if n not in note_groups]\n",
    "    not_group_nodes = list(set(nodes['name']) - set(note_groups))\n",
    "\n",
    "    url = [n for n in not_group_nodes if n[:4] == 'http']\n",
    "    program_nodes = []\n",
    "    note_nodes = []\n",
    "    for u in url:\n",
    "        if \"programs\" in u:\n",
    "            program_nodes.append(u)\n",
    "        elif \"notes\" in u:\n",
    "            note_nodes.append(u)\n",
    "        else:\n",
    "            print(u)\n",
    "\n",
    "    # name_nodes = [n for n in not_group_nodes if '_-_' in n]\n",
    "    # dur_nodes = [n for n in not_group_nodes if n[:3] == 'dur']\n",
    "    # vel_nodes = [n for n in not_group_nodes if n[:3] == 'vel']\n",
    "    # time_nodes = [n for n in not_group_nodes if n[:4] == 'time']\n",
    "    # tempo_nodes = list(set(not_group_nodes) - set(dur_nodes).union(vel_nodes, time_nodes, name_nodes, url))\n",
    "\n",
    "    not_group_url_nodes = list(set(not_group_nodes) - set(url))\n",
    "    name_nodes = []\n",
    "    dur_nodes = []\n",
    "    vel_nodes = []\n",
    "    time_nodes = []\n",
    "    tempo_nodes = []\n",
    "    for n in not_group_url_nodes:\n",
    "        if n[0] == '-' :\n",
    "            name_nodes.append(n)\n",
    "        elif n[:3] == 'dur':\n",
    "            dur_nodes.append(n)\n",
    "        elif n[:3] == 'vel':\n",
    "            vel_nodes.append(n)\n",
    "        elif n[:4] == 'time':\n",
    "            time_nodes.append(n)\n",
    "        else:\n",
    "            tempo_nodes.append(n)\n",
    "\n",
    "    node_categories = {\"note_group\": note_groups,\n",
    "                       \"pitch\": note_nodes,\n",
    "                       \"program\": program_nodes,\n",
    "                       \"MIDI\": name_nodes,\n",
    "                       \"duration\": dur_nodes,\n",
    "                       \"velocity\": vel_nodes,\n",
    "                       \"time_sig\": time_nodes,\n",
    "                       \"tempo\": tempo_nodes\n",
    "                       }\n",
    "    return node_categories\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:04.414759700Z",
     "start_time": "2023-06-18T16:33:04.404750400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['note_group', 'pitch', 'program', 'MIDI', 'duration', 'velocity', 'time_sig', 'tempo'])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_categories = node_cat_dict_giant(nodes)\n",
    "node_categories.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:04.985234200Z",
     "start_time": "2023-06-18T16:33:04.415760400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "nodes_df_complete = pd.read_csv('.\\giantmidi-piano\\complete_csv\\\\nodes_complete.csv')\n",
    "edges_df_complete = pd.read_csv('.\\giantmidi-piano\\complete_csv\\edges_complete.csv')\n",
    "print('Done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:07.114967100Z",
     "start_time": "2023-06-18T16:33:04.986236300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "        Unnamed: 0                                               name  \\\n0                0  -Bach+_Johann_Sebastian+_Das_wohltemperierte_K...   \n1                1                                           g1695254   \n2                2                   http://purl.org/midi-ld/notes/77   \n3                3                                              dur:2   \n4                4                                              vel:6   \n...            ...                                                ...   \n286545      286545                                         g952353344   \n286546      286546                                        g-549989140   \n286547      286547                 http://purl.org/midi-ld/programs/0   \n286548      286548                                                 12   \n286549      286549                                        timesig:4/4   \n\n         node_type  \n0             MIDI  \n1       note_group  \n2            pitch  \n3         duration  \n4         velocity  \n...            ...  \n286545  note_group  \n286546  note_group  \n286547     program  \n286548       tempo  \n286549    time_sig  \n\n[286550 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>name</th>\n      <th>node_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-Bach+_Johann_Sebastian+_Das_wohltemperierte_K...</td>\n      <td>MIDI</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>g1695254</td>\n      <td>note_group</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>http://purl.org/midi-ld/notes/77</td>\n      <td>pitch</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>dur:2</td>\n      <td>duration</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>vel:6</td>\n      <td>velocity</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>286545</th>\n      <td>286545</td>\n      <td>g952353344</td>\n      <td>note_group</td>\n    </tr>\n    <tr>\n      <th>286546</th>\n      <td>286546</td>\n      <td>g-549989140</td>\n      <td>note_group</td>\n    </tr>\n    <tr>\n      <th>286547</th>\n      <td>286547</td>\n      <td>http://purl.org/midi-ld/programs/0</td>\n      <td>program</td>\n    </tr>\n    <tr>\n      <th>286548</th>\n      <td>286548</td>\n      <td>12</td>\n      <td>tempo</td>\n    </tr>\n    <tr>\n      <th>286549</th>\n      <td>286549</td>\n      <td>timesig:4/4</td>\n      <td>time_sig</td>\n    </tr>\n  </tbody>\n</table>\n<p>286550 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_df_complete"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:07.160005600Z",
     "start_time": "2023-06-18T16:33:07.115967400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['note_group__contains__pitch',\n 'note_group__has__duration',\n 'note_group__has__velocity',\n 'MIDI__has__program',\n 'MIDI__has__tempo',\n 'MIDI__has__note_group',\n 'MIDI__in__time_sig']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(edges_df_complete['edge_type']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:07.269097200Z",
     "start_time": "2023-06-18T16:33:07.130980600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'MIDI',\n 'duration',\n 'note_group',\n 'pitch',\n 'program',\n 'tempo',\n 'time_sig',\n 'velocity'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_types = set(nodes_df_complete['node_type'])\n",
    "node_types"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:07.285111Z",
     "start_time": "2023-06-18T16:33:07.270098600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "edge_types = [\"MIDI__has__tempo\",\n",
    "              \"MIDI__in__time_sig\",\n",
    "              \"MIDI__has__program\",\n",
    "              \"MIDI__has__note_group\",\n",
    "              \"note_group__has__velocity\",\n",
    "              \"note_group__has__duration\",\n",
    "              \"note_group__contains__pitch\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:07.347164300Z",
     "start_time": "2023-06-18T16:33:07.286111800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "names_list = flatten_lol(node_categories.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:07.352168Z",
     "start_time": "2023-06-18T16:33:07.302126300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "encoder = Encoder(names_list, n_labels=15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:07.455255800Z",
     "start_time": "2023-06-18T16:33:07.320140700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_nodes took 0.00 secs to run\n",
      "encode_nodes took 0.00 secs to run\n",
      "encode_nodes took 0.00 secs to run\n",
      "encode_nodes took 0.00 secs to run\n",
      "encode_nodes took 0.00 secs to run\n",
      "encode_nodes took 9.91 secs to run\n",
      "encode_nodes took 0.03 secs to run\n",
      "encode_nodes took 0.00 secs to run\n"
     ]
    }
   ],
   "source": [
    "input_node_dict = {node_type: {'x': encoder.\n",
    "                    encode_nodes(nodes_df_complete.\n",
    "                    loc[nodes_df_complete['node_type'] == node_type, ['name']])}\n",
    "                    for node_type in node_types}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:17.516599900Z",
     "start_time": "2023-06-18T16:33:07.454255Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "node_enc_to_idx = {node_type: {encoder.decode_value(node_enc.item()): i for i, node_enc in enumerate(input_node_dict[node_type]['x'])} for node_type in node_types}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:18.816880700Z",
     "start_time": "2023-06-18T16:33:17.517600800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "input_edge_dict = dict()\n",
    "for edge_type in edge_types:\n",
    "    node_type_s, node_type_t = edge_type.split('__')[0], edge_type.split('__')[2]\n",
    "\n",
    "    edge_df = edges_df_complete.loc[edges_df_complete['edge_type'] == edge_type, ['source', 'target']].copy()\n",
    "\n",
    "    edge_df['source'], edge_df['target'] = edge_df['source'].map(node_enc_to_idx[node_type_s]), edge_df['target'].map(node_enc_to_idx[node_type_t])\n",
    "\n",
    "    input_edge_dict[edge_type] = {'edge_index': torch.tensor(edge_df.values).T}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:20.819581Z",
     "start_time": "2023-06-18T16:33:18.817882500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Extract the label of each Midi.\n",
    "midi_val = nodes_df_complete.loc[nodes_df_complete['node_type'] == 'MIDI', ['name']].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:20.834594200Z",
     "start_time": "2023-06-18T16:33:20.819581Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def midi_composer(midi_name: str) -> str:\n",
    "        return midi_name.split('+_')[0].replace('-', \"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:20.862617300Z",
     "start_time": "2023-06-18T16:33:20.834594200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "midi_classes = [midi_composer(s[0]) for s in midi_val]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:20.865620200Z",
     "start_time": "2023-06-18T16:33:20.849606700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Bach', 'Beethoven', 'Carbajo', 'Chopin', 'Czerny', 'Handel',\n       'Haydn', 'Liszt', 'Mozart', 'Rebikov', 'Satie', 'Scarlatti',\n       'Schubert', 'Scriabin', 'Simpson'], dtype='<U9')"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelEncoder()\n",
    "y = torch.from_numpy(lb.fit_transform(midi_classes)) # .type(torch.LongTensor)\n",
    "\n",
    "lb.classes_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:20.880633200Z",
     "start_time": "2023-06-18T16:33:20.864619300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "input_node_dict['MIDI']['y'] = y\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:20.913661100Z",
     "start_time": "2023-06-18T16:33:20.880633200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "H = HeteroData(input_node_dict, **input_edge_dict).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:20.972710900Z",
     "start_time": "2023-06-18T16:33:20.895645800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001B[1mvelocity\u001B[0m={ x=[10, 1] },\n",
      "  \u001B[1mduration\u001B[0m={ x=[61, 1] },\n",
      "  \u001B[1mprogram\u001B[0m={ x=[1, 1] },\n",
      "  \u001B[1mtempo\u001B[0m={ x=[1, 1] },\n",
      "  \u001B[1mtime_sig\u001B[0m={ x=[1, 1] },\n",
      "  \u001B[1mnote_group\u001B[0m={ x=[285642, 1] },\n",
      "  \u001B[1mMIDI\u001B[0m={\n",
      "    x=[750, 1],\n",
      "    y=[750]\n",
      "  },\n",
      "  \u001B[1mpitch\u001B[0m={ x=[84, 1] },\n",
      "  \u001B[1m(MIDI, has, tempo)\u001B[0m={ edge_index=[2, 750] },\n",
      "  \u001B[1m(MIDI, in, time_sig)\u001B[0m={ edge_index=[2, 750] },\n",
      "  \u001B[1m(MIDI, has, program)\u001B[0m={ edge_index=[2, 750] },\n",
      "  \u001B[1m(MIDI, has, note_group)\u001B[0m={ edge_index=[2, 953329] },\n",
      "  \u001B[1m(note_group, has, velocity)\u001B[0m={ edge_index=[2, 434069] },\n",
      "  \u001B[1m(note_group, has, duration)\u001B[0m={ edge_index=[2, 285658] },\n",
      "  \u001B[1m(note_group, contains, pitch)\u001B[0m={ edge_index=[2, 1081559] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(H)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:20.987724100Z",
     "start_time": "2023-06-18T16:33:20.973712400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# To enable 2-way message passing\n",
    "H = T.ToUndirected()(H)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:21.838461100Z",
     "start_time": "2023-06-18T16:33:20.988724900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "H = T.RandomNodeSplit(num_val=0.1, num_test=0.2)(H)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:21.853941500Z",
     "start_time": "2023-06-18T16:33:21.839461700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001B[1mvelocity\u001B[0m={ x=[10, 1] },\n",
      "  \u001B[1mduration\u001B[0m={ x=[61, 1] },\n",
      "  \u001B[1mprogram\u001B[0m={ x=[1, 1] },\n",
      "  \u001B[1mtempo\u001B[0m={ x=[1, 1] },\n",
      "  \u001B[1mtime_sig\u001B[0m={ x=[1, 1] },\n",
      "  \u001B[1mnote_group\u001B[0m={ x=[285642, 1] },\n",
      "  \u001B[1mMIDI\u001B[0m={\n",
      "    x=[750, 1],\n",
      "    y=[750],\n",
      "    train_mask=[750],\n",
      "    val_mask=[750],\n",
      "    test_mask=[750]\n",
      "  },\n",
      "  \u001B[1mpitch\u001B[0m={ x=[84, 1] },\n",
      "  \u001B[1m(MIDI, has, tempo)\u001B[0m={ edge_index=[2, 750] },\n",
      "  \u001B[1m(MIDI, in, time_sig)\u001B[0m={ edge_index=[2, 750] },\n",
      "  \u001B[1m(MIDI, has, program)\u001B[0m={ edge_index=[2, 750] },\n",
      "  \u001B[1m(MIDI, has, note_group)\u001B[0m={ edge_index=[2, 953329] },\n",
      "  \u001B[1m(note_group, has, velocity)\u001B[0m={ edge_index=[2, 434069] },\n",
      "  \u001B[1m(note_group, has, duration)\u001B[0m={ edge_index=[2, 285658] },\n",
      "  \u001B[1m(note_group, contains, pitch)\u001B[0m={ edge_index=[2, 1081559] },\n",
      "  \u001B[1m(tempo, rev_has, MIDI)\u001B[0m={ edge_index=[2, 750] },\n",
      "  \u001B[1m(time_sig, rev_in, MIDI)\u001B[0m={ edge_index=[2, 750] },\n",
      "  \u001B[1m(program, rev_has, MIDI)\u001B[0m={ edge_index=[2, 750] },\n",
      "  \u001B[1m(note_group, rev_has, MIDI)\u001B[0m={ edge_index=[2, 953329] },\n",
      "  \u001B[1m(velocity, rev_has, note_group)\u001B[0m={ edge_index=[2, 434069] },\n",
      "  \u001B[1m(duration, rev_has, note_group)\u001B[0m={ edge_index=[2, 285658] },\n",
      "  \u001B[1m(pitch, rev_contains, note_group)\u001B[0m={ edge_index=[2, 1081559] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(H)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:21.899347900Z",
     "start_time": "2023-06-18T16:33:21.854942800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "GraphModule(\n  (conv2): ModuleDict(\n    (MIDI__has__tempo): SAGEConv((-1, -1), 64, aggr=mean)\n    (MIDI__in__time_sig): SAGEConv((-1, -1), 64, aggr=mean)\n    (MIDI__has__program): SAGEConv((-1, -1), 64, aggr=mean)\n    (MIDI__has__note_group): SAGEConv((-1, -1), 64, aggr=mean)\n    (note_group__has__velocity): SAGEConv((-1, -1), 64, aggr=mean)\n    (note_group__has__duration): SAGEConv((-1, -1), 64, aggr=mean)\n    (note_group__contains__pitch): SAGEConv((-1, -1), 64, aggr=mean)\n    (tempo__rev_has__MIDI): SAGEConv((-1, -1), 64, aggr=mean)\n    (time_sig__rev_in__MIDI): SAGEConv((-1, -1), 64, aggr=mean)\n    (program__rev_has__MIDI): SAGEConv((-1, -1), 64, aggr=mean)\n    (note_group__rev_has__MIDI): SAGEConv((-1, -1), 64, aggr=mean)\n    (velocity__rev_has__note_group): SAGEConv((-1, -1), 64, aggr=mean)\n    (duration__rev_has__note_group): SAGEConv((-1, -1), 64, aggr=mean)\n    (pitch__rev_contains__note_group): SAGEConv((-1, -1), 64, aggr=mean)\n  )\n  (conv1): ModuleDict(\n    (MIDI__has__tempo): SAGEConv((-1, -1), 64, aggr=mean)\n    (MIDI__in__time_sig): SAGEConv((-1, -1), 64, aggr=mean)\n    (MIDI__has__program): SAGEConv((-1, -1), 64, aggr=mean)\n    (MIDI__has__note_group): SAGEConv((-1, -1), 64, aggr=mean)\n    (note_group__has__velocity): SAGEConv((-1, -1), 64, aggr=mean)\n    (note_group__has__duration): SAGEConv((-1, -1), 64, aggr=mean)\n    (note_group__contains__pitch): SAGEConv((-1, -1), 64, aggr=mean)\n    (tempo__rev_has__MIDI): SAGEConv((-1, -1), 64, aggr=mean)\n    (time_sig__rev_in__MIDI): SAGEConv((-1, -1), 64, aggr=mean)\n    (program__rev_has__MIDI): SAGEConv((-1, -1), 64, aggr=mean)\n    (note_group__rev_has__MIDI): SAGEConv((-1, -1), 64, aggr=mean)\n    (velocity__rev_has__note_group): SAGEConv((-1, -1), 64, aggr=mean)\n    (duration__rev_has__note_group): SAGEConv((-1, -1), 64, aggr=mean)\n    (pitch__rev_contains__note_group): SAGEConv((-1, -1), 64, aggr=mean)\n  )\n  (lin): Linear(64, 15, bias=True)\n)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, drop_layer: bool=False, drop_rate: float=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "        self.drop = drop_layer\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        if self.drop:\n",
    "            x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "            x = self.conv2(x, edge_index).relu()\n",
    "            x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "            x = self.conv1(x, edge_index)\n",
    "\n",
    "        else:\n",
    "            x = self.conv1(x, edge_index).relu()\n",
    "            x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Custom weight initialization\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.zeros_(self.conv1.bias)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.zeros_(self.conv2.bias)\n",
    "\n",
    "model = GCN(hidden_channels=64, out_channels=len(set(lb.classes_)), drop_layer=True, drop_rate = .2)\n",
    "model = to_hetero(model, H.metadata(), aggr='sum')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:21.915956200Z",
     "start_time": "2023-06-18T16:33:21.872325Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "optimizer_name = \"Adam\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:21.962264500Z",
     "start_time": "2023-06-18T16:33:21.916957500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(H.x_dict, H.edge_index_dict)\n",
    "    mask = H['MIDI'].train_mask\n",
    "    loss = F.cross_entropy(out['MIDI'][mask], H['MIDI'].y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:21.968271200Z",
     "start_time": "2023-06-18T16:33:21.932346900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(H.x_dict, H.edge_index_dict)['MIDI'].argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for mask in [H['MIDI'].train_mask, H['MIDI'].val_mask, H['MIDI'].test_mask]:\n",
    "        accs.append(int((pred[mask] == H['MIDI'].y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:33:21.968271200Z",
     "start_time": "2023-06-18T16:33:21.947359500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1535597.7500, Train: 0.0590, Val: 0.0667, Test: 0.0467\n",
      "Epoch: 002, Loss: 1113934.2500, Train: 0.0686, Val: 0.0800, Test: 0.0600\n",
      "Epoch: 003, Loss: 771434.5625, Train: 0.0648, Val: 0.0800, Test: 0.0600\n",
      "Epoch: 004, Loss: 1160760.3750, Train: 0.0667, Val: 0.0800, Test: 0.0400\n",
      "Epoch: 005, Loss: 1248206.0000, Train: 0.0781, Val: 0.1067, Test: 0.0533\n",
      "Epoch: 006, Loss: 1539415.5000, Train: 0.0610, Val: 0.1200, Test: 0.0533\n",
      "Epoch: 007, Loss: 1177181.7500, Train: 0.0800, Val: 0.0400, Test: 0.0533\n",
      "Epoch: 008, Loss: 1715433.3750, Train: 0.0571, Val: 0.1200, Test: 0.0667\n",
      "Epoch: 009, Loss: 1739024.8750, Train: 0.0743, Val: 0.0667, Test: 0.0933\n",
      "Epoch: 010, Loss: 1524728.0000, Train: 0.0819, Val: 0.0267, Test: 0.0800\n",
      "Epoch: 011, Loss: 2017966.5000, Train: 0.0686, Val: 0.0667, Test: 0.0933\n",
      "Epoch: 012, Loss: 1862559.6250, Train: 0.0610, Val: 0.0400, Test: 0.1067\n",
      "Epoch: 013, Loss: 1468728.6250, Train: 0.0590, Val: 0.0800, Test: 0.0800\n",
      "Epoch: 014, Loss: 1245456.2500, Train: 0.0629, Val: 0.1200, Test: 0.0667\n",
      "Epoch: 015, Loss: 1287358.1250, Train: 0.0648, Val: 0.0933, Test: 0.0600\n",
      "Epoch: 016, Loss: 1454025.6250, Train: 0.0610, Val: 0.1200, Test: 0.0600\n",
      "Epoch: 017, Loss: 1349546.8750, Train: 0.0648, Val: 0.0933, Test: 0.0600\n",
      "Epoch: 018, Loss: 1457555.8750, Train: 0.0914, Val: 0.1067, Test: 0.0667\n",
      "Epoch: 019, Loss: 1342968.6250, Train: 0.0743, Val: 0.0667, Test: 0.0600\n",
      "Epoch: 020, Loss: 1047552.5000, Train: 0.0686, Val: 0.0667, Test: 0.0600\n",
      "Epoch: 021, Loss: 1020693.1875, Train: 0.0705, Val: 0.0667, Test: 0.0733\n",
      "Epoch: 022, Loss: 982300.1250, Train: 0.0762, Val: 0.0667, Test: 0.0667\n",
      "Epoch: 023, Loss: 903217.5000, Train: 0.0914, Val: 0.0533, Test: 0.0800\n",
      "Epoch: 024, Loss: 798817.4375, Train: 0.0971, Val: 0.0800, Test: 0.0667\n",
      "Epoch: 025, Loss: 786217.0625, Train: 0.0610, Val: 0.0800, Test: 0.0533\n",
      "Epoch: 026, Loss: 886955.0625, Train: 0.0762, Val: 0.0400, Test: 0.0533\n",
      "Epoch: 027, Loss: 840215.1250, Train: 0.0724, Val: 0.0800, Test: 0.0467\n",
      "Epoch: 028, Loss: 1088667.7500, Train: 0.0743, Val: 0.0800, Test: 0.0600\n",
      "Epoch: 029, Loss: 1013371.5000, Train: 0.0952, Val: 0.0533, Test: 0.0733\n",
      "Epoch: 030, Loss: 868533.3125, Train: 0.0743, Val: 0.0400, Test: 0.0733\n",
      "Epoch: 031, Loss: 746777.0625, Train: 0.0876, Val: 0.0667, Test: 0.0867\n",
      "Epoch: 032, Loss: 697142.3750, Train: 0.0724, Val: 0.0267, Test: 0.1267\n",
      "Epoch: 033, Loss: 615421.1250, Train: 0.0914, Val: 0.0933, Test: 0.0867\n",
      "Epoch: 034, Loss: 919601.3750, Train: 0.0686, Val: 0.0400, Test: 0.0600\n",
      "Epoch: 035, Loss: 773993.0625, Train: 0.0857, Val: 0.0133, Test: 0.0933\n",
      "Epoch: 036, Loss: 566825.4375, Train: 0.0819, Val: 0.0133, Test: 0.0600\n",
      "Epoch: 037, Loss: 536241.7500, Train: 0.0895, Val: 0.0267, Test: 0.0533\n",
      "Epoch: 038, Loss: 529816.3750, Train: 0.0552, Val: 0.1333, Test: 0.0867\n",
      "Epoch: 039, Loss: 541762.2500, Train: 0.0781, Val: 0.0800, Test: 0.0667\n",
      "Epoch: 040, Loss: 577394.0625, Train: 0.0724, Val: 0.0400, Test: 0.0600\n",
      "Epoch: 041, Loss: 1104155.2500, Train: 0.0648, Val: 0.0533, Test: 0.0733\n",
      "Epoch: 042, Loss: 605520.5000, Train: 0.0914, Val: 0.1200, Test: 0.1067\n",
      "Epoch: 043, Loss: 601682.1875, Train: 0.0971, Val: 0.0800, Test: 0.1000\n",
      "Epoch: 044, Loss: 712665.0625, Train: 0.0895, Val: 0.0933, Test: 0.0667\n",
      "Epoch: 045, Loss: 578617.0625, Train: 0.0762, Val: 0.0667, Test: 0.0667\n",
      "Epoch: 046, Loss: 641488.9375, Train: 0.1086, Val: 0.0800, Test: 0.1067\n",
      "Epoch: 047, Loss: 518277.0000, Train: 0.0952, Val: 0.1200, Test: 0.1067\n",
      "Epoch: 048, Loss: 582281.5000, Train: 0.0876, Val: 0.0933, Test: 0.1133\n",
      "Epoch: 049, Loss: 648982.6250, Train: 0.1181, Val: 0.1200, Test: 0.0867\n",
      "Epoch: 050, Loss: 464926.9375, Train: 0.1410, Val: 0.1333, Test: 0.1000\n",
      "Epoch: 051, Loss: 538665.5000, Train: 0.1295, Val: 0.1200, Test: 0.1000\n",
      "Epoch: 052, Loss: 553194.2500, Train: 0.1181, Val: 0.0933, Test: 0.0867\n",
      "Epoch: 053, Loss: 561835.6875, Train: 0.1124, Val: 0.0933, Test: 0.0800\n",
      "Epoch: 054, Loss: 629728.7500, Train: 0.0990, Val: 0.0400, Test: 0.0600\n",
      "Epoch: 055, Loss: 471625.3750, Train: 0.0724, Val: 0.0933, Test: 0.0467\n",
      "Epoch: 056, Loss: 408566.5938, Train: 0.1010, Val: 0.1200, Test: 0.0667\n",
      "Epoch: 057, Loss: 1036389.6875, Train: 0.1086, Val: 0.1733, Test: 0.0933\n",
      "Epoch: 058, Loss: 453397.8750, Train: 0.1124, Val: 0.0933, Test: 0.0667\n",
      "Epoch: 059, Loss: 564181.0000, Train: 0.1048, Val: 0.1600, Test: 0.0800\n",
      "Epoch: 060, Loss: 415445.3125, Train: 0.0724, Val: 0.0533, Test: 0.0867\n",
      "Epoch: 061, Loss: 414406.7812, Train: 0.1219, Val: 0.2000, Test: 0.1000\n",
      "Epoch: 062, Loss: 462033.8438, Train: 0.0610, Val: 0.1067, Test: 0.1000\n",
      "Epoch: 063, Loss: 456303.9062, Train: 0.1029, Val: 0.0800, Test: 0.1000\n",
      "Epoch: 064, Loss: 406305.9375, Train: 0.0952, Val: 0.0667, Test: 0.1133\n",
      "Epoch: 065, Loss: 612451.2500, Train: 0.1048, Val: 0.0667, Test: 0.0600\n",
      "Epoch: 066, Loss: 378766.5625, Train: 0.0743, Val: 0.0667, Test: 0.0733\n",
      "Epoch: 067, Loss: 475739.9375, Train: 0.1162, Val: 0.1067, Test: 0.1400\n",
      "Epoch: 068, Loss: 394659.5312, Train: 0.0876, Val: 0.0667, Test: 0.0933\n",
      "Epoch: 069, Loss: 412694.5938, Train: 0.0819, Val: 0.0933, Test: 0.0467\n",
      "Epoch: 070, Loss: 405515.6250, Train: 0.0971, Val: 0.1333, Test: 0.1333\n",
      "Epoch: 071, Loss: 508686.0938, Train: 0.0686, Val: 0.0667, Test: 0.0933\n",
      "Epoch: 072, Loss: 429348.0312, Train: 0.0762, Val: 0.0800, Test: 0.0600\n",
      "Epoch: 073, Loss: 479367.9375, Train: 0.0800, Val: 0.0800, Test: 0.0467\n",
      "Epoch: 074, Loss: 357305.9375, Train: 0.0705, Val: 0.0533, Test: 0.0733\n",
      "Epoch: 075, Loss: 401255.6875, Train: 0.0800, Val: 0.0800, Test: 0.1200\n",
      "Epoch: 076, Loss: 530769.1875, Train: 0.0857, Val: 0.0800, Test: 0.0933\n",
      "Epoch: 077, Loss: 443693.1562, Train: 0.1162, Val: 0.1733, Test: 0.1733\n",
      "Epoch: 078, Loss: 364873.2500, Train: 0.0724, Val: 0.0800, Test: 0.0933\n",
      "Epoch: 079, Loss: 500048.2188, Train: 0.0952, Val: 0.1600, Test: 0.0800\n",
      "Epoch: 080, Loss: 360368.6562, Train: 0.1257, Val: 0.1733, Test: 0.1400\n",
      "Epoch: 081, Loss: 415966.4062, Train: 0.1505, Val: 0.1067, Test: 0.1600\n",
      "Epoch: 082, Loss: 307519.3438, Train: 0.1448, Val: 0.1733, Test: 0.1800\n",
      "Epoch: 083, Loss: 442094.5000, Train: 0.1143, Val: 0.1733, Test: 0.1000\n",
      "Epoch: 084, Loss: 382118.8750, Train: 0.1448, Val: 0.1067, Test: 0.1600\n",
      "Epoch: 085, Loss: 361645.4062, Train: 0.0876, Val: 0.1200, Test: 0.1067\n",
      "Epoch: 086, Loss: 358094.7500, Train: 0.0629, Val: 0.0667, Test: 0.0867\n",
      "Epoch: 087, Loss: 289869.3125, Train: 0.0895, Val: 0.1200, Test: 0.1200\n",
      "Epoch: 088, Loss: 302513.0000, Train: 0.0686, Val: 0.0800, Test: 0.0467\n",
      "Epoch: 089, Loss: 438733.5000, Train: 0.0610, Val: 0.0800, Test: 0.0800\n",
      "Epoch: 090, Loss: 320922.0000, Train: 0.0933, Val: 0.1333, Test: 0.1133\n",
      "Epoch: 091, Loss: 369181.6875, Train: 0.1257, Val: 0.1067, Test: 0.1067\n",
      "Epoch: 092, Loss: 407923.2188, Train: 0.1086, Val: 0.1333, Test: 0.1333\n",
      "Epoch: 093, Loss: 394885.8438, Train: 0.1390, Val: 0.0933, Test: 0.1333\n",
      "Epoch: 094, Loss: 322045.3125, Train: 0.1276, Val: 0.1067, Test: 0.1067\n",
      "Epoch: 095, Loss: 237784.1562, Train: 0.1333, Val: 0.1067, Test: 0.1067\n",
      "Epoch: 096, Loss: 586969.8750, Train: 0.0686, Val: 0.1200, Test: 0.0867\n",
      "Epoch: 097, Loss: 412840.4062, Train: 0.0895, Val: 0.0533, Test: 0.0667\n",
      "Epoch: 098, Loss: 268878.5000, Train: 0.1257, Val: 0.0533, Test: 0.0467\n",
      "Epoch: 099, Loss: 362417.0312, Train: 0.1219, Val: 0.0533, Test: 0.0533\n",
      "Epoch: 100, Loss: 283538.7812, Train: 0.1238, Val: 0.1600, Test: 0.1067\n",
      "Epoch: 101, Loss: 402874.4062, Train: 0.1429, Val: 0.0800, Test: 0.1000\n",
      "Epoch: 102, Loss: 252283.5312, Train: 0.1657, Val: 0.0933, Test: 0.1667\n",
      "Epoch: 103, Loss: 318291.6875, Train: 0.1295, Val: 0.0800, Test: 0.1533\n",
      "Epoch: 104, Loss: 259994.8750, Train: 0.0933, Val: 0.0800, Test: 0.0800\n",
      "Epoch: 105, Loss: 482031.5000, Train: 0.0857, Val: 0.0800, Test: 0.0733\n",
      "Epoch: 106, Loss: 404234.6875, Train: 0.0933, Val: 0.1067, Test: 0.0867\n",
      "Epoch: 107, Loss: 289222.9688, Train: 0.1181, Val: 0.0533, Test: 0.1200\n",
      "Epoch: 108, Loss: 343657.5938, Train: 0.1067, Val: 0.1067, Test: 0.1267\n",
      "Epoch: 109, Loss: 378498.1250, Train: 0.1790, Val: 0.1867, Test: 0.2000\n",
      "Epoch: 110, Loss: 241025.0625, Train: 0.0933, Val: 0.0667, Test: 0.1000\n",
      "Epoch: 111, Loss: 356613.0625, Train: 0.1448, Val: 0.0933, Test: 0.1400\n",
      "Epoch: 112, Loss: 225201.4531, Train: 0.1695, Val: 0.1733, Test: 0.1333\n",
      "Epoch: 113, Loss: 269596.7500, Train: 0.2076, Val: 0.2400, Test: 0.1800\n",
      "Epoch: 114, Loss: 212553.2812, Train: 0.1619, Val: 0.2533, Test: 0.1600\n",
      "Epoch: 115, Loss: 289200.3438, Train: 0.1429, Val: 0.1600, Test: 0.1267\n",
      "Epoch: 116, Loss: 258311.1562, Train: 0.1352, Val: 0.0933, Test: 0.1000\n",
      "Epoch: 117, Loss: 190050.7812, Train: 0.1238, Val: 0.1067, Test: 0.1200\n",
      "Epoch: 118, Loss: 222498.6875, Train: 0.1314, Val: 0.1200, Test: 0.1000\n",
      "Epoch: 119, Loss: 288330.4688, Train: 0.1238, Val: 0.1467, Test: 0.1133\n",
      "Epoch: 120, Loss: 239962.4375, Train: 0.0933, Val: 0.1067, Test: 0.0867\n",
      "Epoch: 121, Loss: 292986.5000, Train: 0.1219, Val: 0.1467, Test: 0.1133\n",
      "Epoch: 122, Loss: 229758.4688, Train: 0.1371, Val: 0.1600, Test: 0.1467\n",
      "Epoch: 123, Loss: 213902.9062, Train: 0.0914, Val: 0.1067, Test: 0.1067\n",
      "Epoch: 124, Loss: 178368.3125, Train: 0.1333, Val: 0.1200, Test: 0.1667\n",
      "Epoch: 125, Loss: 224594.7812, Train: 0.1429, Val: 0.1467, Test: 0.1267\n",
      "Epoch: 126, Loss: 159427.9844, Train: 0.1295, Val: 0.1200, Test: 0.1400\n",
      "Epoch: 127, Loss: 245774.9062, Train: 0.1333, Val: 0.1467, Test: 0.1533\n",
      "Epoch: 128, Loss: 302684.1875, Train: 0.1638, Val: 0.1867, Test: 0.1467\n",
      "Epoch: 129, Loss: 147173.6875, Train: 0.1924, Val: 0.2400, Test: 0.1867\n",
      "Epoch: 130, Loss: 275617.1562, Train: 0.1276, Val: 0.1733, Test: 0.1267\n",
      "Epoch: 131, Loss: 206213.8281, Train: 0.1314, Val: 0.1867, Test: 0.1333\n",
      "Epoch: 132, Loss: 241130.0469, Train: 0.1314, Val: 0.1200, Test: 0.0867\n",
      "Epoch: 133, Loss: 187903.1250, Train: 0.1162, Val: 0.1600, Test: 0.1333\n",
      "Epoch: 134, Loss: 174250.3438, Train: 0.1314, Val: 0.1733, Test: 0.1267\n",
      "Epoch: 135, Loss: 215499.1250, Train: 0.1371, Val: 0.0667, Test: 0.1400\n",
      "Epoch: 136, Loss: 132988.4375, Train: 0.1276, Val: 0.0933, Test: 0.1333\n",
      "Epoch: 137, Loss: 214088.0625, Train: 0.1390, Val: 0.0933, Test: 0.1000\n",
      "Epoch: 138, Loss: 135888.2812, Train: 0.1124, Val: 0.1067, Test: 0.0867\n",
      "Epoch: 139, Loss: 137451.8438, Train: 0.1810, Val: 0.1867, Test: 0.2067\n",
      "Epoch: 140, Loss: 236386.0312, Train: 0.1295, Val: 0.0933, Test: 0.1267\n",
      "Epoch: 141, Loss: 100352.6172, Train: 0.1505, Val: 0.0667, Test: 0.1333\n",
      "Epoch: 142, Loss: 152391.2656, Train: 0.1257, Val: 0.0933, Test: 0.1133\n",
      "Epoch: 143, Loss: 166331.7812, Train: 0.2190, Val: 0.1067, Test: 0.1867\n",
      "Epoch: 144, Loss: 114248.6328, Train: 0.1467, Val: 0.0667, Test: 0.1400\n",
      "Epoch: 145, Loss: 122254.1094, Train: 0.1086, Val: 0.0667, Test: 0.1067\n",
      "Epoch: 146, Loss: 229463.2656, Train: 0.1295, Val: 0.0800, Test: 0.1267\n",
      "Epoch: 147, Loss: 143888.8750, Train: 0.1562, Val: 0.0667, Test: 0.1400\n",
      "Epoch: 148, Loss: 92186.5781, Train: 0.1600, Val: 0.1467, Test: 0.1333\n",
      "Epoch: 149, Loss: 139766.1094, Train: 0.1505, Val: 0.1333, Test: 0.1000\n",
      "Epoch: 150, Loss: 147430.0312, Train: 0.1410, Val: 0.1067, Test: 0.1133\n",
      "Epoch: 151, Loss: 133883.6250, Train: 0.1562, Val: 0.1733, Test: 0.1133\n",
      "Epoch: 152, Loss: 83387.5859, Train: 0.1695, Val: 0.1467, Test: 0.1600\n",
      "Epoch: 153, Loss: 126008.8750, Train: 0.1848, Val: 0.1067, Test: 0.1800\n",
      "Epoch: 154, Loss: 110491.5469, Train: 0.0971, Val: 0.0667, Test: 0.1200\n",
      "Epoch: 155, Loss: 137966.8594, Train: 0.1829, Val: 0.2667, Test: 0.1867\n",
      "Epoch: 156, Loss: 126395.4766, Train: 0.2133, Val: 0.2000, Test: 0.2200\n",
      "Epoch: 157, Loss: 72287.1953, Train: 0.1581, Val: 0.1867, Test: 0.1667\n",
      "Epoch: 158, Loss: 124722.2969, Train: 0.1505, Val: 0.2000, Test: 0.1533\n",
      "Epoch: 159, Loss: 169280.2656, Train: 0.1086, Val: 0.1200, Test: 0.1200\n",
      "Epoch: 160, Loss: 105176.8281, Train: 0.1676, Val: 0.1600, Test: 0.1533\n",
      "Epoch: 161, Loss: 127116.3906, Train: 0.1010, Val: 0.1067, Test: 0.1733\n",
      "Epoch: 162, Loss: 103945.8203, Train: 0.0933, Val: 0.0667, Test: 0.1333\n",
      "Epoch: 163, Loss: 151186.8906, Train: 0.1657, Val: 0.1867, Test: 0.1667\n",
      "Epoch: 164, Loss: 133426.7031, Train: 0.1257, Val: 0.1733, Test: 0.1333\n",
      "Epoch: 165, Loss: 189600.5469, Train: 0.1524, Val: 0.1333, Test: 0.1133\n",
      "Epoch: 166, Loss: 133478.8594, Train: 0.1829, Val: 0.2133, Test: 0.1800\n",
      "Epoch: 167, Loss: 115035.7422, Train: 0.1219, Val: 0.1867, Test: 0.1333\n",
      "Epoch: 168, Loss: 125647.8203, Train: 0.1790, Val: 0.1600, Test: 0.1733\n",
      "Epoch: 169, Loss: 112098.3047, Train: 0.1238, Val: 0.1600, Test: 0.1400\n",
      "Epoch: 170, Loss: 141010.5156, Train: 0.1486, Val: 0.0933, Test: 0.1267\n",
      "Epoch: 171, Loss: 161143.2500, Train: 0.2171, Val: 0.0933, Test: 0.1667\n",
      "Epoch: 172, Loss: 123162.8047, Train: 0.1733, Val: 0.1333, Test: 0.1333\n",
      "Epoch: 173, Loss: 58829.5117, Train: 0.2210, Val: 0.1733, Test: 0.1733\n",
      "Epoch: 174, Loss: 110394.1406, Train: 0.1162, Val: 0.0400, Test: 0.1467\n",
      "Epoch: 175, Loss: 91939.8984, Train: 0.1581, Val: 0.1867, Test: 0.1333\n",
      "Epoch: 176, Loss: 67169.1094, Train: 0.1524, Val: 0.2000, Test: 0.2067\n",
      "Epoch: 177, Loss: 107540.6016, Train: 0.1314, Val: 0.0667, Test: 0.0867\n",
      "Epoch: 178, Loss: 292073.9062, Train: 0.0781, Val: 0.1067, Test: 0.0467\n",
      "Epoch: 179, Loss: 115035.3047, Train: 0.2076, Val: 0.2000, Test: 0.2133\n",
      "Epoch: 180, Loss: 131826.0156, Train: 0.1638, Val: 0.1200, Test: 0.1400\n",
      "Epoch: 181, Loss: 110958.4141, Train: 0.1619, Val: 0.1600, Test: 0.1533\n",
      "Epoch: 182, Loss: 160019.4062, Train: 0.1371, Val: 0.1467, Test: 0.0933\n",
      "Epoch: 183, Loss: 69292.4219, Train: 0.1886, Val: 0.0800, Test: 0.1867\n",
      "Epoch: 184, Loss: 127163.1641, Train: 0.2362, Val: 0.2400, Test: 0.1800\n",
      "Epoch: 185, Loss: 99081.9297, Train: 0.1219, Val: 0.1067, Test: 0.0867\n",
      "Epoch: 186, Loss: 112427.0469, Train: 0.1086, Val: 0.0667, Test: 0.1200\n",
      "Epoch: 187, Loss: 214426.1562, Train: 0.1276, Val: 0.1067, Test: 0.1000\n",
      "Epoch: 188, Loss: 87152.2109, Train: 0.1448, Val: 0.1067, Test: 0.1400\n",
      "Epoch: 189, Loss: 144531.6094, Train: 0.1810, Val: 0.1200, Test: 0.1467\n",
      "Epoch: 190, Loss: 167757.5781, Train: 0.1867, Val: 0.1200, Test: 0.1533\n",
      "Epoch: 191, Loss: 154212.8594, Train: 0.1562, Val: 0.1333, Test: 0.1400\n",
      "Epoch: 192, Loss: 116900.0859, Train: 0.1333, Val: 0.1200, Test: 0.1200\n",
      "Epoch: 193, Loss: 131501.0156, Train: 0.1752, Val: 0.1333, Test: 0.1667\n",
      "Epoch: 194, Loss: 131943.2656, Train: 0.1771, Val: 0.0667, Test: 0.1400\n",
      "Epoch: 195, Loss: 84559.2344, Train: 0.0610, Val: 0.0533, Test: 0.0933\n",
      "Epoch: 196, Loss: 87025.0312, Train: 0.1067, Val: 0.0667, Test: 0.1067\n",
      "Epoch: 197, Loss: 140238.0938, Train: 0.1410, Val: 0.0933, Test: 0.1467\n",
      "Epoch: 198, Loss: 124742.1406, Train: 0.1181, Val: 0.0933, Test: 0.1333\n",
      "Epoch: 199, Loss: 119584.6875, Train: 0.1886, Val: 0.0800, Test: 0.1733\n",
      "Epoch: 200, Loss: 142994.2969, Train: 0.1295, Val: 0.1200, Test: 0.1133\n",
      "Epoch: 201, Loss: 107724.0469, Train: 0.0857, Val: 0.1067, Test: 0.1000\n",
      "Epoch: 202, Loss: 118107.7812, Train: 0.1448, Val: 0.1733, Test: 0.1533\n",
      "Epoch: 203, Loss: 73695.9375, Train: 0.1848, Val: 0.1867, Test: 0.2133\n",
      "Epoch: 204, Loss: 107314.1250, Train: 0.0667, Val: 0.0933, Test: 0.0800\n",
      "Epoch: 205, Loss: 71390.1875, Train: 0.1676, Val: 0.2267, Test: 0.1467\n",
      "Epoch: 206, Loss: 87559.1172, Train: 0.1067, Val: 0.1467, Test: 0.0800\n",
      "Epoch: 207, Loss: 152863.7656, Train: 0.1105, Val: 0.1600, Test: 0.1133\n",
      "Epoch: 208, Loss: 75249.0000, Train: 0.2057, Val: 0.1733, Test: 0.1733\n",
      "Epoch: 209, Loss: 96914.6562, Train: 0.1333, Val: 0.0667, Test: 0.1000\n",
      "Epoch: 210, Loss: 105894.4453, Train: 0.1390, Val: 0.0800, Test: 0.1267\n",
      "Epoch: 211, Loss: 165348.3281, Train: 0.1981, Val: 0.1600, Test: 0.2667\n",
      "Epoch: 212, Loss: 83520.9375, Train: 0.2057, Val: 0.2133, Test: 0.2067\n",
      "Epoch: 213, Loss: 155390.4062, Train: 0.1600, Val: 0.1067, Test: 0.1467\n",
      "Epoch: 214, Loss: 161316.8906, Train: 0.1257, Val: 0.1467, Test: 0.1333\n",
      "Epoch: 215, Loss: 139606.9531, Train: 0.1371, Val: 0.1867, Test: 0.1333\n",
      "Epoch: 216, Loss: 86384.9922, Train: 0.2190, Val: 0.2667, Test: 0.2400\n",
      "Epoch: 217, Loss: 110102.3047, Train: 0.1448, Val: 0.1600, Test: 0.1333\n",
      "Epoch: 218, Loss: 94595.7188, Train: 0.1638, Val: 0.1467, Test: 0.1333\n",
      "Epoch: 219, Loss: 105641.1328, Train: 0.1524, Val: 0.1600, Test: 0.1333\n",
      "Epoch: 220, Loss: 104851.0156, Train: 0.1371, Val: 0.1467, Test: 0.1000\n",
      "Epoch: 221, Loss: 128070.7344, Train: 0.2495, Val: 0.1333, Test: 0.2733\n",
      "Epoch: 222, Loss: 125197.2969, Train: 0.2419, Val: 0.2533, Test: 0.1867\n",
      "Epoch: 223, Loss: 71073.1719, Train: 0.1352, Val: 0.0933, Test: 0.1267\n",
      "Epoch: 224, Loss: 96279.7422, Train: 0.1790, Val: 0.1733, Test: 0.2067\n",
      "Epoch: 225, Loss: 68293.0859, Train: 0.1086, Val: 0.0000, Test: 0.0800\n",
      "Epoch: 226, Loss: 57678.6055, Train: 0.1771, Val: 0.1733, Test: 0.2333\n",
      "Epoch: 227, Loss: 100274.6328, Train: 0.1429, Val: 0.1333, Test: 0.1467\n",
      "Epoch: 228, Loss: 107829.1250, Train: 0.2095, Val: 0.1733, Test: 0.1867\n",
      "Epoch: 229, Loss: 95697.7578, Train: 0.2267, Val: 0.1067, Test: 0.2200\n",
      "Epoch: 230, Loss: 54383.0391, Train: 0.2248, Val: 0.3067, Test: 0.1933\n",
      "Epoch: 231, Loss: 69136.1641, Train: 0.1771, Val: 0.1867, Test: 0.1800\n",
      "Epoch: 232, Loss: 55390.8320, Train: 0.2267, Val: 0.1867, Test: 0.1733\n",
      "Epoch: 233, Loss: 133646.8125, Train: 0.2419, Val: 0.1733, Test: 0.2333\n",
      "Epoch: 234, Loss: 128511.1172, Train: 0.1886, Val: 0.2000, Test: 0.2133\n",
      "Epoch: 235, Loss: 117954.5078, Train: 0.1524, Val: 0.1733, Test: 0.1267\n",
      "Epoch: 236, Loss: 83520.1250, Train: 0.1048, Val: 0.0667, Test: 0.1067\n",
      "Epoch: 237, Loss: 60540.5430, Train: 0.2438, Val: 0.2267, Test: 0.1533\n",
      "Epoch: 238, Loss: 45864.8906, Train: 0.1295, Val: 0.1067, Test: 0.1067\n",
      "Epoch: 239, Loss: 104283.6016, Train: 0.2514, Val: 0.2800, Test: 0.2333\n",
      "Epoch: 240, Loss: 66431.6875, Train: 0.1276, Val: 0.1867, Test: 0.1800\n",
      "Epoch: 241, Loss: 50612.6797, Train: 0.2095, Val: 0.2133, Test: 0.1933\n",
      "Epoch: 242, Loss: 60101.0742, Train: 0.1162, Val: 0.1600, Test: 0.1267\n",
      "Epoch: 243, Loss: 44569.2500, Train: 0.1448, Val: 0.1600, Test: 0.1067\n",
      "Epoch: 244, Loss: 69971.8516, Train: 0.1429, Val: 0.1867, Test: 0.1400\n",
      "Epoch: 245, Loss: 146025.5469, Train: 0.2552, Val: 0.2400, Test: 0.2133\n",
      "Epoch: 246, Loss: 59625.1562, Train: 0.2667, Val: 0.3467, Test: 0.2800\n",
      "Epoch: 247, Loss: 66634.4766, Train: 0.1200, Val: 0.0800, Test: 0.1200\n",
      "Epoch: 248, Loss: 73312.8828, Train: 0.2971, Val: 0.2533, Test: 0.2733\n",
      "Epoch: 249, Loss: 49010.5352, Train: 0.1314, Val: 0.1200, Test: 0.1267\n",
      "Epoch: 250, Loss: 75246.1328, Train: 0.1333, Val: 0.0533, Test: 0.0933\n",
      "Epoch: 251, Loss: 71170.3125, Train: 0.2190, Val: 0.1867, Test: 0.2533\n",
      "Epoch: 252, Loss: 57433.1211, Train: 0.2057, Val: 0.2000, Test: 0.2400\n",
      "Epoch: 253, Loss: 55771.8594, Train: 0.1524, Val: 0.1467, Test: 0.1467\n",
      "Epoch: 254, Loss: 55526.5625, Train: 0.2362, Val: 0.2400, Test: 0.2067\n",
      "Epoch: 255, Loss: 65994.7188, Train: 0.1448, Val: 0.1333, Test: 0.1000\n",
      "Epoch: 256, Loss: 63840.7695, Train: 0.1771, Val: 0.1600, Test: 0.1267\n",
      "Epoch: 257, Loss: 46008.0781, Train: 0.2324, Val: 0.1867, Test: 0.2000\n",
      "Epoch: 258, Loss: 90411.6406, Train: 0.1448, Val: 0.1067, Test: 0.1200\n",
      "Epoch: 259, Loss: 87681.8438, Train: 0.2190, Val: 0.1867, Test: 0.2200\n",
      "Epoch: 260, Loss: 80882.0547, Train: 0.1695, Val: 0.1733, Test: 0.0933\n",
      "Epoch: 261, Loss: 86972.2812, Train: 0.1448, Val: 0.2000, Test: 0.0933\n",
      "Epoch: 262, Loss: 64083.0703, Train: 0.1657, Val: 0.1067, Test: 0.1267\n",
      "Epoch: 263, Loss: 76782.1953, Train: 0.1714, Val: 0.1467, Test: 0.1400\n",
      "Epoch: 264, Loss: 50904.7305, Train: 0.1181, Val: 0.1067, Test: 0.1533\n",
      "Epoch: 265, Loss: 59184.3086, Train: 0.2781, Val: 0.2933, Test: 0.2333\n",
      "Epoch: 266, Loss: 70712.9844, Train: 0.2057, Val: 0.1067, Test: 0.2000\n",
      "Epoch: 267, Loss: 79943.3594, Train: 0.1886, Val: 0.1867, Test: 0.2000\n",
      "Epoch: 268, Loss: 59502.2539, Train: 0.1467, Val: 0.1467, Test: 0.1267\n",
      "Epoch: 269, Loss: 54222.9102, Train: 0.1467, Val: 0.1200, Test: 0.1667\n",
      "Epoch: 270, Loss: 50897.4531, Train: 0.1238, Val: 0.0933, Test: 0.1000\n",
      "Epoch: 271, Loss: 87939.0781, Train: 0.1943, Val: 0.2533, Test: 0.1867\n",
      "Epoch: 272, Loss: 65818.9609, Train: 0.1486, Val: 0.2400, Test: 0.1800\n",
      "Epoch: 273, Loss: 45621.3945, Train: 0.1581, Val: 0.1333, Test: 0.1600\n",
      "Epoch: 274, Loss: 47864.8672, Train: 0.1810, Val: 0.1600, Test: 0.1933\n",
      "Epoch: 275, Loss: 70713.6797, Train: 0.1962, Val: 0.2267, Test: 0.1933\n",
      "Epoch: 276, Loss: 45800.3242, Train: 0.2305, Val: 0.2400, Test: 0.2133\n",
      "Epoch: 277, Loss: 48831.8867, Train: 0.2057, Val: 0.2800, Test: 0.1867\n",
      "Epoch: 278, Loss: 55543.0859, Train: 0.1638, Val: 0.2267, Test: 0.1867\n",
      "Epoch: 279, Loss: 61693.7695, Train: 0.1581, Val: 0.2133, Test: 0.1600\n",
      "Epoch: 280, Loss: 55395.1289, Train: 0.2400, Val: 0.1467, Test: 0.2467\n",
      "Epoch: 281, Loss: 37350.3672, Train: 0.2114, Val: 0.1333, Test: 0.1800\n",
      "Epoch: 282, Loss: 44333.8164, Train: 0.1771, Val: 0.1200, Test: 0.2067\n",
      "Epoch: 283, Loss: 61837.6758, Train: 0.2000, Val: 0.1333, Test: 0.2267\n",
      "Epoch: 284, Loss: 47574.7773, Train: 0.2571, Val: 0.2533, Test: 0.2133\n",
      "Epoch: 285, Loss: 38689.7227, Train: 0.2267, Val: 0.2267, Test: 0.1733\n",
      "Epoch: 286, Loss: 58262.2656, Train: 0.2514, Val: 0.1733, Test: 0.2733\n",
      "Epoch: 287, Loss: 68870.0000, Train: 0.1543, Val: 0.0933, Test: 0.1333\n",
      "Epoch: 288, Loss: 72122.8594, Train: 0.2400, Val: 0.2400, Test: 0.2067\n",
      "Epoch: 289, Loss: 41662.2812, Train: 0.2076, Val: 0.1333, Test: 0.1733\n",
      "Epoch: 290, Loss: 37410.1602, Train: 0.2895, Val: 0.1600, Test: 0.2333\n",
      "Epoch: 291, Loss: 34383.5273, Train: 0.1790, Val: 0.1333, Test: 0.1467\n",
      "Epoch: 292, Loss: 47043.9727, Train: 0.2781, Val: 0.2000, Test: 0.2133\n",
      "Epoch: 293, Loss: 51856.5273, Train: 0.2724, Val: 0.1600, Test: 0.2600\n",
      "Epoch: 294, Loss: 42114.2891, Train: 0.2381, Val: 0.1467, Test: 0.2200\n",
      "Epoch: 295, Loss: 46622.2500, Train: 0.1943, Val: 0.1467, Test: 0.1667\n",
      "Epoch: 296, Loss: 36866.1523, Train: 0.2705, Val: 0.2800, Test: 0.2867\n",
      "Epoch: 297, Loss: 50036.1055, Train: 0.2438, Val: 0.2000, Test: 0.2467\n",
      "Epoch: 298, Loss: 34369.2852, Train: 0.1505, Val: 0.1600, Test: 0.1467\n",
      "Epoch: 299, Loss: 37353.3477, Train: 0.3333, Val: 0.3067, Test: 0.3400\n",
      "Epoch: 300, Loss: 56879.0820, Train: 0.1943, Val: 0.2667, Test: 0.2333\n",
      "Epoch: 301, Loss: 53425.4414, Train: 0.2229, Val: 0.2533, Test: 0.2333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11520\\1247691125.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m801\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[0mtrain_acc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_acc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_acc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11520\\3699364296.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mH\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mx_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mH\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0medge_index_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mmask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mH\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'MIDI'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_mask\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcross_entropy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'MIDI'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mH\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'MIDI'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "acc_lists = {'train': [], 'val': [], 'test': []}\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(1, 801):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "\n",
    "    loss_list.append(loss)\n",
    "\n",
    "    acc_lists['train'].append(train_acc)\n",
    "    acc_lists['val'].append(val_acc)\n",
    "    acc_lists['test'].append(test_acc)\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "          f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:37:04.445833400Z",
     "start_time": "2023-06-18T16:33:21.963265600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_4graphs(loss_list, acc_lists)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask = H['MIDI'].test_mask\n",
    "\n",
    "predicted = model(H.x_dict, H.edge_index_dict)['MIDI'].argmax(dim=-1)[mask]\n",
    "\n",
    "predicted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_true=lb.inverse_transform(H['MIDI'].y[mask].to('cpu')), y_pred=lb.inverse_transform(predicted.to('cpu')), cmap='bone', normalize='true')\n",
    "\n",
    "disp.figure_.set_size_inches(20, 16)\n",
    "disp.ax_.set_title('GiantMIDI-Piano Composer')\n",
    "\n",
    "disp.figure_.savefig(\".\\giantmidi-piano\\giantmidi_conf_matrix1.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "winsound.Beep(400, 700)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
